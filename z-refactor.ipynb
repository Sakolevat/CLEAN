{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "from helper.dataloader import *\n",
    "from helper.model import *\n",
    "from helper.utils import *\n",
    "from helper.losses import *\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from helper.distance_map import get_dist_map\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "dtype = torch.float32\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split test data by number of EC occurance in train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train_data = \"uniref10_train_split_0\"\n",
    "args_test_data = \"uniref10_test_split_1_curate\"\n",
    "id_ec_tr, ec_id_dict_tr = get_ec_id_dict('./data/' + args_train_data + '.csv')\n",
    "id_ec_te, ec_id_dict_te = get_ec_id_dict('./data/' + args_test_data + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_name = './data/' + args_test_data + '.csv'\n",
    "csv_file = open(csv_name)\n",
    "csvreader = csv.reader(csv_file, delimiter='\\t')\n",
    " \n",
    "n_bins = 5\n",
    "out_writer_lst = []\n",
    "for bin in range(n_bins):\n",
    "    subset_file = './data/subset/' + \\\n",
    "        args_test_data + '_subset' + str(bin) + '.csv'\n",
    "    out_file = open(subset_file, 'w', newline='')\n",
    "    csvwriter = csv.writer(out_file, delimiter='\\t')\n",
    "    out_writer_lst.append(csvwriter)\n",
    "\n",
    "\n",
    "for i, row in enumerate(csvreader):\n",
    "    if i == 0:\n",
    "        for bin in range(n_bins):\n",
    "            out_writer_lst[bin].writerow(row)\n",
    "    else:\n",
    "        true_ec_lst = row[1].split(';')\n",
    "        id_count_lst = [len(ec_id_dict_tr[ec]) for ec in true_ec_lst]\n",
    "        id_count_ec = np.max(id_count_lst)\n",
    "\n",
    "        if id_count_ec >= 2 and id_count_ec < 5:\n",
    "            out_writer_lst[0].writerow(row)\n",
    "        elif id_count_ec >= 5 and id_count_ec < 20:\n",
    "            out_writer_lst[1].writerow(row)\n",
    "        elif id_count_ec >= 20 and id_count_ec < 50:\n",
    "            out_writer_lst[2].writerow(row)\n",
    "        elif id_count_ec >= 50 and id_count_ec < 100:\n",
    "            out_writer_lst[3].writerow(row)\n",
    "        else:\n",
    "            out_writer_lst[4].writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EC frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train_data = \"uniref100_train_split_0\"\n",
    "args_test_data = \"uniref10_test_split_1_curate\"\n",
    "id_ec_tr, ec_id_dict_tr = get_ec_id_dict('./data/' + args_train_data + '.csv')\n",
    "id_ec_te, ec_id_dict_te = get_ec_id_dict('./data/' + args_test_data + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "EC_freq = []\n",
    "for key in ec_id_dict_tr.keys():\n",
    "    per_EC_id_count = len(ec_id_dict_tr[key])\n",
    "    EC_freq.append(per_EC_id_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "_ = plt.hist(EC_freq, bins=30)\n",
    "_ = plt.yscale('log')\n",
    "_ = plt.xticks(np.arange(0, 6200, 500)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### multiple postive multiple negative dataset/dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = 9; n_neg = 20\n",
    "def get_dataloader(dist_map, id_ec, ec_id, args):\n",
    "    params = {\n",
    "        'batch_size': 5000,\n",
    "        'shuffle': True,\n",
    "    }\n",
    "    negative = mine_hard_negative(dist_map, 50)\n",
    "    train_data = MultiPosNeg_dataset_with_mine_EC(\n",
    "        id_ec, ec_id, negative, n_pos, n_neg)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, **params)\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train_data = \"uniref10_train_split_1\"\n",
    "args_hidden_dim = 512\n",
    "args_out_dim = 128\n",
    "id_ec, ec_id_dict = get_ec_id_dict('./data/' + args_train_data + '.csv')\n",
    "ec_id = {key: list(ec_id_dict[key]) for key in ec_id_dict.keys()}\n",
    "model = LayerNormNet(args_hidden_dim, args_out_dim, device, dtype)\n",
    "checkpoint = torch.load('./model/uniref10_split1_final.pth')\n",
    "dist_map = pickle.load(\n",
    "    open('./data/distance_map/uniref10_train_split_1.pkl', 'rb'))\n",
    "args = []\n",
    "train_loader = get_dataloader(dist_map, id_ec, ec_id, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pos: 9 ; number of neg: 20\n",
      "size of one batch: torch.Size([4223, 30, 1280])\n"
     ]
    }
   ],
   "source": [
    "for batch, data in enumerate(train_loader):\n",
    "    print(\"number of pos:\", n_pos, \"; number of neg:\", n_neg)\n",
    "    print(\"size of one batch:\", data.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-pair loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is: 6.433290481567383\n",
      "the grad of fc1 layer is: tensor([[-4.8914e-03, -2.0231e-03,  4.1318e-03,  ..., -1.6695e-03,\n",
      "          1.7229e-03, -2.7476e-03],\n",
      "        [ 1.0904e-02, -1.3277e-02, -3.2206e-03,  ..., -1.7161e-03,\n",
      "         -2.7405e-03, -7.5002e-03],\n",
      "        [-3.0050e-03, -6.1143e-03, -3.8868e-03,  ...,  1.4125e-03,\n",
      "          7.5213e-03,  6.8182e-03],\n",
      "        ...,\n",
      "        [ 4.4915e-03,  7.5347e-03, -3.2780e-03,  ...,  4.0461e-05,\n",
      "          3.3811e-03, -7.3777e-03],\n",
      "        [ 4.0005e-03, -3.6585e-03, -3.7010e-03,  ..., -8.1621e-03,\n",
      "         -5.8130e-04, -1.2024e-03],\n",
      "        [-3.7205e-03, -1.0952e-03, -5.4550e-03,  ..., -4.0717e-04,\n",
      "          3.1118e-04,  9.3065e-03]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "args_hidden_dim = 512\n",
    "args_out_dim = 128\n",
    "model = LayerNormNet(args_hidden_dim, args_out_dim, device, dtype)\n",
    "checkpoint = torch.load('./model/uniref10_split1_final.pth')\n",
    "model.zero_grad()\n",
    "# the input to the model will be of size [bsz, (1+n_pos+n_neg), 1280]\n",
    "n_pos = 1; n_neg = 30; bsz = 400; temp = 0.1\n",
    "n_all = 1 + n_pos + n_neg\n",
    "dummy_input = torch.randn(bsz, n_all, 1280).to(device, dtype)\n",
    "model_emb = model(dummy_input)\n",
    "loss = NPairLoss(model_emb, temp)\n",
    "loss.backward()\n",
    "print(\"loss is:\", loss.item())\n",
    "print(\"the grad of fc1 layer is:\", model.fc1.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SupCon Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is: 6.3981146812438965\n",
      "the grad of fc1 layer is: tensor([[-0.0002,  0.0006, -0.0036,  ..., -0.0042,  0.0003,  0.0011],\n",
      "        [ 0.0089, -0.0080,  0.0055,  ..., -0.0009,  0.0020,  0.0026],\n",
      "        [ 0.0008, -0.0011,  0.0004,  ...,  0.0025,  0.0010,  0.0015],\n",
      "        ...,\n",
      "        [ 0.0014, -0.0014,  0.0008,  ..., -0.0012, -0.0020, -0.0017],\n",
      "        [-0.0009,  0.0002, -0.0023,  ..., -0.0014,  0.0006, -0.0015],\n",
      "        [-0.0046,  0.0005,  0.0002,  ..., -0.0016, -0.0012, -0.0003]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "seed_everything()\n",
    "args_hidden_dim = 512\n",
    "args_out_dim = 128\n",
    "model = LayerNormNet(args_hidden_dim, args_out_dim, device, dtype)\n",
    "checkpoint = torch.load('./model/uniref10_split1_final.pth')\n",
    "model.zero_grad()\n",
    "# the input to the model will be of size [bsz, (1+n_pos+n_neg), 1280]\n",
    "n_pos = 10; n_neg = 30; bsz = 400; temp = 0.1\n",
    "n_all = 1 + n_pos + n_neg\n",
    "dummy_input = torch.randn(bsz, n_all, 1280).to(device, dtype)\n",
    "model_emb = model(dummy_input)\n",
    "loss = SupConHardLoss(model_emb, temp, n_pos)\n",
    "loss.backward()\n",
    "print(\"loss is:\", loss.item())\n",
    "print(\"the grad of fc1 layer is:\", model.fc1.weight.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "386218770bb7053658aedbdb94aaaba888065d92b04918111f39a883f4943438"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
