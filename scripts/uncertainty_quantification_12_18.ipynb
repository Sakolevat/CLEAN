{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25824/127727820.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistance_map\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhelper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'helper'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from helper.model import *\n",
    "from helper.utils import *\n",
    "from helper.distance_map import *\n",
    "from helper.evaluate import *\n",
    "import pandas as pd\n",
    "import warnings\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = warn\n",
    "\n",
    "args_train_data = \"uniref100_full\"\n",
    "args_test_data = \"new\"\n",
    "args_model_name = \"split100_ensemble/split100_\"  \n",
    "args_weight_path = \"uncertainty_ensemble/\" \n",
    "args_N_models = 8\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "dtype = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 157/5242 [00:07<03:55, 21.58it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_33104/2809926377.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mmodel_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mmodel_i\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0memb_train_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_i\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mesm_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec_id_dict_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0memb_test_lst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_embedding_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid_ec_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_i\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\OneDrive - Cornell University\\Desktop\\Zhao_lab\\PENCiL\\helper\\utils.py\u001b[0m in \u001b[0;36mesm_embedding\u001b[1;34m(ec_id_dict, device, dtype)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec_id_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mids_for_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec_id_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mesm_to_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mload_esm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids_for_query\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mesm_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mesm_emb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mesm_to_cat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mesm_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\OneDrive - Cornell University\\Desktop\\Zhao_lab\\PENCiL\\helper\\utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mec\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec_id_dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[0mids_for_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mec_id_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mec\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mesm_to_cat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mload_esm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mids_for_query\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mesm_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mesm_emb\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mesm_to_cat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mesm_emb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\OneDrive - Cornell University\\Desktop\\Zhao_lab\\PENCiL\\helper\\utils.py\u001b[0m in \u001b[0;36mload_esm\u001b[1;34m(lookup)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_esm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m     \u001b[0mesm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mformat_esm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data/esm_weights/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlookup\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mesm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    710\u001b[0m                     \u001b[0mopened_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_position\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 712\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    713\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_load\u001b[1;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[0;32m   1044\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_loaded_sparse_tensors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[1;34m(saved_id)\u001b[0m\n\u001b[0;32m   1014\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1015\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1016\u001b[1;33m             \u001b[0mload_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1017\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1018\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloaded_storages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[1;34m(dtype, numel, key, location)\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mf'data/{key}'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 997\u001b[1;33m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_UntypedStorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_untyped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    998\u001b[0m         \u001b[1;31m# TODO: Once we decide to break serialization FC, we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m         \u001b[1;31m# stop wrapping with _TypedStorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mstorage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m         \u001b[0mstorage_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dtype_to_storage_type_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[0mstorage_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'.'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstorage_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstorage_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrap_storage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstorage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\canal\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "id_ec_train, ec_id_dict_train = get_ec_id_dict('./data/'+args_train_data+'.csv')\n",
    "id_ec_test, _ = get_ec_id_dict('./data/'+args_test_data+'.csv')\n",
    "# keep models output embeddings\n",
    "emb_train_lst = []\n",
    "emb_test_lst = []\n",
    "\n",
    "for i in (range(args_N_models)):\n",
    "    model_i = LayerNormNet(512, 128, device, dtype)\n",
    "    model_name_i = args_model_name + str(i)\n",
    "    print(\"loading model:\", model_name_i)\n",
    "    checkpoint = torch.load('./model/' + model_name_i + '.pth')\n",
    "    model_i.load_state_dict(checkpoint)\n",
    "    model_i.eval()\n",
    "    emb_train_lst.append(model_i(esm_embedding(ec_id_dict_train, device, dtype)))\n",
    "    emb_test_lst.append(model_embedding_test(id_ec_test, model_i, device, dtype)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5242/5242 [01:12<00:00, 72.76it/s] \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# load id ec from tran and test\n",
    "id_ec_train, ec_id_dict_train = get_ec_id_dict(\n",
    "    './data/' + args_train_data + '.csv')\n",
    "id_ec_test, _ = get_ec_id_dict(\n",
    "    './data/' + args_test_data + '.csv')\n",
    "\n",
    "# load model\n",
    "if False:\n",
    "    # no model used for pretrained embedding\n",
    "    model = lambda *args: args[0]\n",
    "else:\n",
    "    model = LayerNormNet(512, 128, device, dtype)\n",
    "    checkpoint = torch.load('./model/' + args_model_name + '.pth')\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.eval()\n",
    "\n",
    "# compute distance map\n",
    "emb_train = model(esm_embedding(ec_id_dict_train, device, dtype))\n",
    "emb_test = model_embedding_test(id_ec_test, model, device, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding sizes for train and test: torch.Size([241025, 128]) torch.Size([392, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5242/5242 [00:00<00:00, 17200.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating eval distance map, between 392 test ids and 5242 train EC cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "392it [00:00, 1095.56it/s]\n"
     ]
    }
   ],
   "source": [
    "eval_dist = get_dist_map_test(\n",
    "        emb_train, emb_test, ec_id_dict_train, id_ec_test, \n",
    "        device, dtype, dot=False)\n",
    "eval_df = pd.DataFrame.from_dict(eval_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_probs(out_filename, pred_type=\"_maxsep\"):\n",
    "    file_name = out_filename+pred_type\n",
    "    result = open(file_name+'.csv', 'r')\n",
    "    csvreader = csv.reader(result, delimiter=',')\n",
    "    pred_probs = []\n",
    "    for row in csvreader:\n",
    "        preds_ec_lst = []\n",
    "        preds_with_dist = row[1:]\n",
    "        probs = torch.zeros(len(preds_with_dist))\n",
    "        count = 0\n",
    "        \n",
    "        for pred_ec_dist in preds_with_dist:\n",
    "            # get EC number 3.5.2.6 from EC:3.5.2.6/10.8359\n",
    "            ec_i = - float(pred_ec_dist.split(\":\")[1].split(\"/\")[1])\n",
    "            probs[count] = ec_i\n",
    "            #preds_ec_lst.append(probs)\n",
    "            count += 1\n",
    "        # sigmoid of the negative distances \n",
    "        # probs = (1 - torch.exp(-1/probs)) / (1 + torch.exp(-1/probs))\n",
    "        # probs = probs/torch.sum(probs)\n",
    "        probs = torch.nn.functional.softmax(probs)\n",
    "        pred_probs.append(probs)\n",
    "        \n",
    "    return pred_probs\n",
    "\n",
    "\n",
    "def get_pred_dist(out_filename, pred_type=\"_maxsep\"):\n",
    "    file_name = out_filename+pred_type\n",
    "    result = open(file_name+'.csv', 'r')\n",
    "    csvreader = csv.reader(result, delimiter=',')\n",
    "    pred_probs = []\n",
    "    for row in csvreader:\n",
    "        preds_ec_lst = []\n",
    "        preds_with_dist = row[1:]\n",
    "        probs = torch.zeros(len(preds_with_dist))\n",
    "        count = 0\n",
    "        \n",
    "        for pred_ec_dist in preds_with_dist:\n",
    "            # get EC number 3.5.2.6 from EC:3.5.2.6/10.8359\n",
    "            ec_i = float(pred_ec_dist.split(\":\")[1].split(\"/\")[1])\n",
    "            probs[count] = ec_i\n",
    "            #preds_ec_lst.append(probs)\n",
    "            count += 1\n",
    "        # sigmoid of the negative distances \n",
    "       \n",
    "        \n",
    "        pred_probs.append(probs)\n",
    "    return pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_filename = './eval/' + args_test_data\n",
    "write_max_sep_choices(eval_df, out_filename, first_grad=True, use_max_grad=False)\n",
    "pred_label = get_pred_labels(out_filename, pred_type='_maxsep')\n",
    "pred_probs= get_pred_probs(out_filename, pred_type='_maxsep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ec_pos_dict(mlb, true_label, pred_label):\n",
    "    ec_list = []\n",
    "    pos_list = []\n",
    "    for i in range(len(true_label)):\n",
    "        ec_list += list(mlb.inverse_transform(mlb.transform([true_label[i]]))[0])\n",
    "        pos_list += list(np.nonzero(mlb.transform([true_label[i]]))[1])\n",
    "    for i in range(len(pred_label)):\n",
    "        ec_list += list(mlb.inverse_transform(mlb.transform([pred_label[i]]))[0])\n",
    "        pos_list += list(np.nonzero(mlb.transform([pred_label[i]]))[1])\n",
    "    label_pos_dict = {}\n",
    "    for i in range(len(ec_list)):\n",
    "        ec, pos = ec_list[i], pos_list[i]\n",
    "        label_pos_dict[ec] = pos\n",
    "        \n",
    "    return label_pos_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ EC calling results using maximum separation ############\n",
      "---------------------------------------------------------------------------\n",
      ">>> total samples: 392 | total ec 177 |\n",
      "precision | recall | F1 | AUC | accuracy\n",
      "0.59406 , 0.48509 , 0.50715 , 0.7418711 , 0.5051\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.DataFrame.from_dict(eval_dist)\n",
    "# write the top 10 closest EC to _top10.csv\n",
    "out_filename = './eval/' + args_test_data\n",
    "# _ = write_top10_choices(eval_df, out_filename)\n",
    "# maximum separation results\n",
    "write_max_sep_choices(eval_df, out_filename,\n",
    "                        first_grad=True,\n",
    "                        use_max_grad=False)\n",
    "# get preds and true labels\n",
    "pred_label = get_pred_labels(out_filename, pred_type='_maxsep')\n",
    "true_label, all_label = get_true_labels('./data/'+args_test_data)\n",
    "pre, rec, f1, roc, acc = get_eval_metrics(pred_label, true_label, all_label)\n",
    "print(\"############ EC calling results using maximum separation ############\")\n",
    "print('-' * 75)\n",
    "print(f'>>> total samples: {len(true_label)} | total ec {len(all_label)} |\\n'\n",
    "        f'precision | recall | F1 | AUC | accuracy' )\n",
    "print( f'{pre:.5} , {rec:.5} , {f1:.5} , {roc:.7} , {acc:.5}')\n",
    "print('-' * 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ Maximum separation w correct AUC ############\n",
      "---------------------------------------------------------------------------\n",
      ">>> total samples: 392 | total ec 177 |\n",
      "precision | recall | F1 | AUC | accuracy\n",
      "0.59406 , 0.48509 , 0.50715 , 0.7419487 , 0.5051\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "eval_df = pd.DataFrame.from_dict(eval_dist)\n",
    "# write the top 10 closest EC to _top10.csv\n",
    "out_filename = './eval/' + args_test_data\n",
    "# _ = write_top10_choices(eval_df, out_filename)\n",
    "# maximum separation results\n",
    "write_max_sep_choices(eval_df, out_filename, first_grad=True, use_max_grad=False)\n",
    "# get preds and true labels\n",
    "pred_label = get_pred_labels(out_filename, pred_type='_maxsep')\n",
    "pred_probs = get_pred_probs(out_filename, pred_type='_maxsep')\n",
    "true_label, all_label = get_true_labels('./data/'+args_test_data)\n",
    "pre, rec, f1, roc, acc = get_eval_metrics_new(\n",
    "    pred_label, pred_probs, true_label, all_label)\n",
    "print(\"############ Maximum separation w correct AUC ############\")\n",
    "print('-' * 75)\n",
    "print(f'>>> total samples: {len(true_label)} | total ec {len(all_label)} |\\n'\n",
    "        f'precision | recall | F1 | AUC | accuracy' )\n",
    "print( f'{pre:.5} , {rec:.5} , {f1:.5} , {roc:.7} , {acc:.5}')\n",
    "print('-' * 75)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "386218770bb7053658aedbdb94aaaba888065d92b04918111f39a883f4943438"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
